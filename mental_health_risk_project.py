# -*- coding: utf-8 -*-
"""Mental Health Risk Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vI_S-ycnqA_JYpJcZKf4NBJ43Ge1Pi2-
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, learning_curve, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score, roc_curve, RocCurveDisplay, auc
)
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE, SMOTENC
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import validation_curve
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf
import os
import random
import sys
import joblib

df = pd.read_csv("mental_health2425v8in.csv")
df.head()

df.describe()

"""**Interpretation:** The df.describe() output provides a summary of 49 numerical columns from a dataset of 579 U.S. college students, showing descriptive statistics for key mental health and demographic variables. Overall, the statistics provide insight into the distribution, central tendency, and variability of depression-related symptoms and other health indicators in the dataset."""

df.info()

"""**Interpretation:** The output of df.info() shows that the dataset contains 579 records (rows) and 118 columns. Among these:

* 49 columns are numerical (int64), likely representing encoded survey responses, scores (e.g., PHQ-9, GAD-7), and other binary or ordinal variables.

* 69 columns are of type object, typically indicating text-based data such as categorical responses, names of conditions, or survey answers that haven’t been encoded yet.

* The total memory usage is about 534 KB

**DATA CLEANING AND PREPARATION**
"""

# Dropping Unnecessary Columns
data= df.drop(['phq9_1', 'phq9_2', 'phq9_3', 'phq9_4', 'phq9_5', 'phq9_6','phq9_7', 'phq9_8', 'phq9_9','phq9_1NUM', 'phq9_2NUM', 'phq9_3NUM',
               'phq9_4NUM', 'phq9_5NUM','phq9_6 NUM','phq9_7NUM','phq9_8NUM', 'phq9_9NUM','gad7_1', 'gad7_2', 'gad7_3', 'gad7_4', 'gad7_5', 'gad7_6',
               'gad7_7', 'gad7_1NUM', 'gad7_2NUM', 'gad7_3NUM', 'gad7_4NUM', 'gad7_5NUM','gad7_6NUM', 'gad7_7NUM','acha_12months_any_1',
               'acha_12months_any_2', 'acha_12months_any_3', 'acha_12months_any_4','acha_12months_any_5','acha_12months_any_6',
               'acha_12months_any_7','acha_12months_any_8','acha_12months_any_9','acha_12months_any_10', 'acha_12months_any_11',
               'acha_12months_any_12','acha_12months_any_13', 'acha_12months_any_14','acha_12months_any_15','acha_12months_any_16',
               'acha_12months_any_17','acha_12months_any_18','acha_12months_any_19','acha_12months_any_20','acha_12months_any_21',
               'acha_12months_any_22','acha_12months_any_23','acha_12months_any_24','acha_12months_any_25',
               'acha_12months_any_26','acha_12months_any_27','acha_12months_any_28','acha_12months_any_29','acha_12months_any_1NUM',
               'acha_12months_any_2NUM','acha_12months_any_3NUM','acha_12months_any_4NUM','acha_12months_any_5NUM','acha_12months_any_6NUM',
               'acha_12months_any_7NUM','acha_12months_any_8NUM','acha_12months_any_9NUM','acha_12months_any_10NUM','acha_12months_any_11NUM',
               'acha_12months_any_12NUM','acha_12months_any_13NUM','acha_12months_any_14NUM','acha_12months_any_15NUM','acha_12months_any_16NUM',
               'acha_12months_any_17NUM','acha_12months_any_18NUM','acha_12months_any_19NUM','acha_12months_any_20NUM','acha_12months_any_21NUM',
               'acha_12months_any_22NUM','acha_12months_any_23NUM','acha_12months_any_24NUM','acha_12months_any_25NUM','acha_12months_any_26NUM',
               'acha_12months_any_27NUM','acha_12months_any_28NUM','acha_12months_any_29NUM','international','fulltime',
               'acha_services_1','acha_services_3',], axis=1)

# Rename column names
data.rename(columns={
    'acha_depression': 'Previously_Diagnosed_Depression',
    'acha_services_2': 'In_Therapy_for_Depression',
    'acha_12months_any_comp': 'Total_Health_Issues_Score',
    'acha_12months_times_1': 'Hopeless',
    'acha_12months_times_2': 'Overwhelmed',
    'acha_12months_times_3': 'Exhausted (not physical)',
    'acha_12months_times_4': 'Very sad',
    'acha_12months_times_5': 'Depressed - hard to function',
    'acha_12months_times_6': 'Seriously considered suicide',
    'acha_12months_times_7': 'Attempted suicide'
}, inplace=True)

# Calculate Age
data['Age'] = 2022 - data['year_1']

"""**Interpretation:** The Age of each respondent is calculated by subtracting the value in the year_1 column from the year 2022. The year_1 variable represents the respondent's year of birth, so this operation estimates their age at the time of data analysis or survey administration. This derived Age variable is important for subsequent analyses, as age can be a significant factor in understanding trends related to mental health severity, anxiety, and other health-related variables among college students."""

# The column was mapped to numeric values as General_Health for analysis
df_data2 = {
    'Poor': 1, 'Fair': 2, 'Good': 3, 'Very Good': 4, 'Excellent': 5
}
data['General_Health'] = data['general_health'].map(df_data2)
data.drop(columns=['general_health'], inplace=True)

# Columns were transformed from categorical frequency responses into a numeric
hopeless = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Hopeless'] = data['Hopeless'].map(hopeless)

overwhelmed = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Overwhelmed'] = data['Overwhelmed'].map(overwhelmed)

exhausted = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}

data['Exhausted (not physical)'] = data['Exhausted (not physical)'].map(exhausted)

very_sad = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Very sad'] = data['Very sad'].map(very_sad)

depressed = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Depressed - hard to function'] = data['Depressed - hard to function'].map(depressed)

considered_suicide = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Seriously considered suicide'] = data['Seriously considered suicide'].map(considered_suicide)

attempted_suicide = {
    "Never": 0,
    "1-2 times": 1,
    "3-4 times": 2,
    "5-6 times": 3,
    "7-8 times": 4,
    "9-10 times": 5,
    "11 or more times": 6
}
data['Attempted suicide'] = data['Attempted suicide'].map(attempted_suicide)

# Convert 'sex' column to : Male as 0 and Female as 1
data['sex'] = data['sex'].map({'Male': 0, 'Female': 1})

# Mapping the PHQ-9 severity categories to numerical scores for easier analysis and modeling
phq9_severity_score = {
    'Minimal depression': 0,
    'Mild depression': 1,
    'Moderate depression': 2,
    'Moderately severe depression': 3,
    'Severe depression': 4
}
data['Depression_Severity_Score'] = data['phq9_severity'].map(phq9_severity_score)

# Mapping the GAD-7 anxiety severity categories to numerical scores
Anxiety_severity_score = {
    'minimal anxiety': 0,
    'mild anxiety': 1,
    'moderate anxiety': 2,
    'severe anxiety': 3
}
data['Anxiety_Severity_Score'] = data['gad7_severity'].map(Anxiety_severity_score)

# Convert 'Previously_Diagnosed_Depression' from categorical 'Yes'/'No' to numerical values
# 'No': 0, 'Yes': 1
# Fill any missing values with 2 to indicate unknown or missing diagnosis status

data['Previously_Diagnosed_Depression'] = data['Previously_Diagnosed_Depression'].map({
    'No': 0,
    'Yes': 1
})
data['Previously_Diagnosed_Depression'] = data['Previously_Diagnosed_Depression'].fillna(2)

# Map 'In_Therapy_for_Depression' from categorical to numeric: 'No' = 0, 'Yes' = 1
# Fill missing values with 2 (indicating unknown/missing)

data['In_Therapy_for_Depression'] = data['In_Therapy_for_Depression'].map({'No': 0, 'Yes': 1}).fillna(2).astype(int)

# Create binary target variable from Anxiety Severity Score
# 0 = No Anxiety, 1 = Mild to Severe Anxiety
data['Anxiety_Severity'] = data['Anxiety_Severity_Score'].apply(lambda x: 0 if x == 0 else 1)

"""**VISUALIZATIONS**"""

plt.figure(figsize=(20, 8))
sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""**Interpretation:** The correlation matrix represents the strength and direction of relationships between variables in the dataset. Mental health symptoms such as feeling "Depressed - hard to function", "Overwhelmed", and "Hopeless" are highly correlated with both depression and anxiety scores, suggesting they are key indicators of psychological distress. Strong correlations are also observed among suicide-related variables like "Seriously considered suicide", "Attempted suicide", and "Depressed - hard to function", reinforcing their close association.

General_Health is negatively correlated with both Depression_Severity_Score or phq9_COMP, indicating that individuals with poorer mental health perceive their general health as worse. Lastly, sex and age show very weak or no meaningful correlation with mental health indicators, implying these demographic factors may not directly influence the severity scores.
"""

emotions = [
    'Hopeless',
    'Overwhelmed',
    'Exhausted (not physical)',
    'Very sad',
    'Depressed - hard to function',
    'Seriously considered suicide',
    'Attempted suicide'
]

long_df = pd.melt(data, value_vars=emotions,
                  var_name='Emotion', value_name='Frequency')

freq_counts = (long_df
               .groupby(['Emotion', 'Frequency'])
               .size()
               .reset_index(name='Count'))

freq_labels = {
    0: '0 = Never',
    1: '1 = 1–2 times',
    2: '2 = 3–4 times',
    3: '3 = 5–6 times',
    4: '4 = 7–8 times',
    5: '5 = 9–10 times',
    6: '6 = 11+ times'
}
freq_counts['Frequency_Label'] = freq_counts['Frequency'].map(freq_labels)

freq_order = [
    '0 = Never', '1 = 1–2 times', '2 = 3–4 times', '3 = 5–6 times',
    '4 = 7–8 times', '5 = 9–10 times', '6 = 11+ times'
]

plt.figure(figsize=(14, 8))
sns.barplot(
    data=freq_counts,
    x='Emotion',
    y='Count',
    hue='Frequency_Label',
    hue_order=freq_order,
    ci=None
)

plt.title('Emotional Distress Frequency Among Participants')
plt.xlabel('Emotional Distress Type')
plt.ylabel('Number of People')
plt.legend(title='Frequency', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretation:** The bar plot illustrates the frequency of various emotional distress experiences among participants. Notably, the majority reported never attempting suicide or seriously considering suicide, indicating these severe responses were less common. However, high frequencies (11+ times) were reported for feelings of exhaustion (not physical), overwhelm, and sadness, suggesting these are more prevalent and persistent emotional struggles. A significant number also reported experiencing feelings like hopelessness and difficulty functioning due to depression multiple times. This distribution highlights that while extreme behaviors like suicidal actions are less frequent, chronic emotional distress is widespread among participants and warrants attention in mental health assessments and interventions."""

data['Race'] = data[['race_1', 'race_2', 'race_3', 'race_4', 'race_5', 'race_6']].bfill(axis=1).iloc[:, 0]
data.drop(columns=['race_1', 'race_2', 'race_3', 'race_4', 'race_5', 'race_6'], inplace=True)
data['Race'] = data['Race'].astype('category')

race_counts = data['Race'].value_counts()

plt.figure(figsize=(4, 4))
plt.pie(race_counts.values, labels=race_counts.index, autopct='%1.1f%%', startangle=160)
plt.title('Race Distribution Among College Students')
plt.axis('equal')
plt.show()

"""**Interpretation:** The pie chart illustrates the racial distribution among college students in the dataset. The majority of students identify as White - not Hispanic (including Middle Eastern), comprising 63.6% of the total population. This is followed by Asian or Pacific Islander students at 17.8%, and Hispanic or Latino students at 10.7%. Black - not Hispanic students make up 7.4%, while those identifying as “Other” represent only 0.5%. This distribution indicates a predominance of White students, with comparatively lower representation from other racial and ethnic groups."""

plt.figure(figsize=(8,6))
sns.violinplot(x='phq9_severity', y='sex', data=data, palette='pastel')
plt.title('Depression Severity by Gender')
plt.xlabel('PHQ-9 Depression Severity Score')
plt.ylabel('Gender')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretation:** The violin plot displays the distribution of PHQ-9 (depression severity scores) across genders. Each depression category-ranging from minimal to severe-is represented by a violin shape, showing the density of participants for each gender (where 0 likely represents males and 1 represents females). Overall, the distribution appears relatively balanced across genders for each severity level, though the female group shows slightly higher density in categories like moderate and moderately severe depression. This suggests that while both genders experience various levels of depression, females may be more represented in higher severity categories. The plot provides a visual cue that depression severity is prevalent across both genders but may impact females more frequently or intensely in certain categories."""

plt.figure(figsize=(10, 6))
sns.boxplot(
    x='gad7_severity',
    y='Age',
    hue='gad7_severity',
    data=data,
    palette='Set2',
    legend=False
)
plt.title('Relationship Between Anxiety Severity and Age', fontsize=14)
plt.xlabel('GAD-7 Severity Level', fontsize=12)
plt.ylabel('Age', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretation:** The boxplot illustrates the relationship between age and GAD-7 (anxiety severity) levels-ranging from minimal to severe anxiety. Across all categories, the age distribution remains fairly consistent, with the median age hovering around 21 to 22 years. The interquartile ranges (IQRs) are also similar, indicating that the majority of respondents in each anxiety category fall within the same age range, typically early adulthood.

There are several outliers above age 30 in all severity groups, suggesting that while most participants are college-aged, a small number of older individuals also experience varying levels of anxiety. Notably, no clear trend emerges between age and increasing anxiety severity; that is, age does not appear to be a strong differentiator across anxiety categories. This suggests that within this college population, anxiety severity is relatively independent of age.
"""

health = {
    1: 'Poor',
    2: 'Fair',
    3: 'Good',
    4: 'Very Good',
    5: 'Excellent'
}

plt.figure(figsize=(8, 6))
sns.countplot(data=data, x='General_Health', hue='Anxiety_Severity', palette='coolwarm')
plt.title('General Health by Anxiety Severity')
plt.xlabel('General Health Score')
plt.ylabel('Count')
plt.legend(title='Anxiety Severity', labels=['No Anxiety', 'Mild-Severe Anxiety'])
tick_labels = [f'{i} ({health[i]})' for i in sorted(health.keys())]
plt.xticks(ticks=range(len(tick_labels)), labels=tick_labels)
plt.tight_layout()
plt.show()

"""**Interpretation:** The bar plot shows the distribution of general health scores among individuals with and without mild to severe anxiety. Overall, individuals with mild-severe anxiety are more prevalent across all health categories compared to those with no anxiety. Notably, the highest number of mild-severe anxiety cases are seen in participants who rated their health as “Good” and “Very Good”, with “Good” showing the peak count.

Interestingly, anxiety is not confined to participants with poor health. Even among those who rate their health as “Very Good” or “Excellent”, many still report experiencing anxiety. Meanwhile, those reporting no anxiety are mostly concentrated in the “Good” and “Very Good” categories as well, though in significantly smaller numbers. This suggests that self-perceived general health alone does not strongly protect against anxiety. While poorer health aligns with increased anxiety risk, many individuals with average or even high health ratings also experience significant anxiety symptoms.
"""

state_avg = data.groupby('state_1')[['phq9_COMP', 'gad7_COMP']].mean().reset_index()
state_avg = state_avg.sort_values(by='phq9_COMP', ascending=False)
state_melted = pd.melt(state_avg, id_vars='state_1', value_vars=['phq9_COMP', 'gad7_COMP'],
                       var_name='Condition', value_name='Average Score')
state_melted['Condition'] = state_melted['Condition'].replace({
    'phq9_COMP': 'Depression (PHQ-9)',
    'gad7_COMP': 'Anxiety (GAD-7)'
})
custom_palette = {
    'Depression (PHQ-9)': '#4B8BBE',
    'Anxiety (GAD-7)': '#FFB000'
}
# Plot
plt.figure(figsize=(14, 6))
sns.barplot(data=state_melted, x='state_1', y='Average Score', hue='Condition', palette=custom_palette)
plt.title('Average Depression and Anxiety Scores by State')
plt.xlabel('State')
plt.ylabel('Average Score')
plt.xticks(rotation=45)
plt.legend(title='Condition')
plt.tight_layout()
plt.show()

"""**Interpretation:** The bar chart displays the average depression (PHQ-9) and anxiety (GAD-7) scores by U.S. state, offering a comparative view of mental health conditions across different regions. Overall, depression scores are generally higher than anxiety scores in most states. States like SD (South Dakota), IN (Indiana), and ME (Maine) show the highest average depression scores, with values nearing or exceeding 15. In contrast, states such as WY (Wyoming), MS (Mississippi), and WV (West Virginia) exhibit significantly higher anxiety scores compared to their depression scores. On the other end, states like HI (Hawaii) and MT (Montana) show relatively low scores for both conditions.

**DESCRIPTIVE STATISTICS**
"""

# For Anxiety using chi2 for categorical values
categorical_vars = [
    'sex',
    'Previously_Diagnosed_Depression',
    'In_Therapy_for_Depression', 'Total_Health_Issues_Score', 'Depression_Severity_Score','Hopeless', 'Overwhelmed', 'Exhausted (not physical)',
       'Very sad', 'Depressed - hard to function',
       'Seriously considered suicide', 'Attempted suicide','Race'
]
results = []
for var in categorical_vars:
    contingency = pd.crosstab(data[var], data['gad7_severity'])
    chi2, p, dof, expected = chi2_contingency(contingency)
    results.append({'Variable': var, 'p-value': p})
chi_square_results = pd.DataFrame(results)
chi_square_results = chi_square_results.sort_values(by='p-value')
chi_square_results['Significant (<0.05)'] = chi_square_results['p-value'] < 0.05
chi_square_results

"""**Interpretation:** The chi-square tests were conducted to examine the association between categorical variables and anxiety severity levels (GAD-7 severity). The chi-square analysis revealed that several emotional distress variables—including “Depressed - hard to function,” “Very sad,” “Exhausted (not physical),” “Hopeless,” and “Overwhelmed”-were highly significantly associated with anxiety severity levels (p < 0.001), suggesting a strong link between these emotional states and elevated anxiety. Additionally, health-related variables such as Total Health Issues Score and a history of depression diagnosis were also significantly related to anxiety severity. Demographic and treatment-related variables like sex and being in therapy for depression showed significant associations as well, indicating potential gender and care-seeking differences in anxiety outcomes. On the other hand, Race and Depression Severity Score did not show statistically significant associations (p > 0.05), suggesting that these factors may not have a strong influence on anxiety severity.

"""

# For Anxiety using Anova for numerical values
numeric_vars = [
    'Age',
    'General_Health'

]
target = 'gad7_severity'
results = []
for var in numeric_vars:
    # Drop missing values
    subset = data[[var, target]].dropna()
    # Group values by each category of target
    groups = [group[var].values for _, group in subset.groupby(target)]
    # Run ANOVA
    if len(groups) > 1:
        f_stat, p = f_oneway(*groups)
        results.append({'Variable': var, 'p-value': p, 'Significant (<0.05)': p < 0.05})
    else:
        results.append({'Variable': var, 'p-value': None, 'Significant (<0.05)': False})
anova_results = pd.DataFrame(results).sort_values(by='p-value', na_position='last').reset_index(drop=True)
anova_results

"""**Interpretation:** The ANOVA analysis examined whether the mean values of numerical variables-Age and General Health-differed significantly across levels of anxiety severity. The results indicated that neither variable showed a statistically significant relationship with anxiety severity (p-values of 0.6188 and 0.9846, respectively). This suggests that, within this dataset, participants' age and their self-reported general health do not significantly influence or vary by the severity of anxiety symptoms. Therefore, these numerical factors may not be strong predictors of anxiety levels in this college student population.

**FEATURE SELECTION AND DATA TRANSFORMATION**
"""

# predictors and target
predictors = [
    'sex', 'Age', 'Race', 'Previously_Diagnosed_Depression',
    'Total_Health_Issues_Score', 'In_Therapy_for_Depression',
    'Hopeless', 'Very sad', 'Depression_Severity_Score',
    'Overwhelmed', 'Exhausted (not physical)',
    'Depressed - hard to function', 'Attempted suicide'
]
target = 'Anxiety_Severity'

# Select data and drop missing rows
model_input_df = data[predictors + [target]].dropna()

X = model_input_df[predictors]
y = model_input_df[target]

# Define categorical and numerical columns properly
categorical_cols = ['sex', 'Race']
numeric_cols = [
    'Age', 'Previously_Diagnosed_Depression', 'In_Therapy_for_Depression',
    'Total_Health_Issues_Score', 'Hopeless', 'Very sad', 'Depression_Severity_Score',
    'Overwhelmed', 'Exhausted (not physical)', 'Depressed - hard to function', 'Attempted suicide'
]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Create preprocessing pipeline
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),
    ("num", StandardScaler(), numeric_cols)
])

# Fit on train, transform train and test
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

"""**HANDLING CLASS IMBALANCE**"""

# Apply SMOTE on training data
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_processed, y_train)

"""**MODELING**

**LOGISTIC REGRESSION**
"""

# Initialize and train a logistic regression model with balanced class weights to handle class imbalance.

log_model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs')
log_model.fit(X_train_res, y_train_res)
y_test_pred = log_model.predict(X_test_processed)
y_train_pred = log_model.predict(X_train_res)

def evaluate_model(name, y_true, y_pred):
    print(f"\n{name} Performance:")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred, average='weighted', zero_division=0))
    print("Recall:", recall_score(y_true, y_pred, average='weighted', zero_division=0))
    print("F1 Score:", f1_score(y_true, y_pred, average='weighted', zero_division=0))
    print("\nClassification Report:\n", classification_report(y_true, y_pred, zero_division=0))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

    # Plot the confusion matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='YlGnBu')
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

evaluate_model("Logistic Regression (Test)", y_test, y_test_pred)
evaluate_model("Logistic Regression (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The Logistic Regression model shows solid and consistent performance on both training and test datasets. On the test set, it achieved an accuracy of approximately 79%, with precision and recall values close to 80%, indicating balanced effectiveness in identifying both classes. The model performs slightly better at detecting class 1 (higher recall and precision) than class 0, but overall maintains good classification metrics. Similarly, on the training set, the accuracy and F1 score remain around 80%, with balanced precision and recall, suggesting the model generalizes well without significant overfitting. The confusion matrices show the model correctly classifies the majority of instances, though some misclassifications exist, particularly with class 0. Overall, Logistic Regression provides reliable and stable predictive performance for this classification task."""

# Initialize Logistic Regression model with increased max iterations,
# lbfgs solver, and balanced class weights to handle class imbalance

log_reg = LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced')

# Define hyperparameter grid for tuning 'C' (regularization strength) and penalty type
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
}

# using 5-fold cross-validation, optimizing for weighted F1 score
grid_search = GridSearchCV(
    estimator=log_reg,
    param_grid=param_grid,
    scoring='f1_weighted',
    cv=5,
    n_jobs=-1,
    verbose=1
)

# Fit grid search on the training data
grid_search.fit(X_train_res, y_train_res)
print("Best parameters:", grid_search.best_params_)
print("Best F1 score:", grid_search.best_score_)

best_log_model = grid_search.best_estimator_
y_test_pred = best_log_model.predict(X_test_processed)
y_train_pred = best_log_model.predict(X_train_res)

evaluate_model("Tuned Logistic Regression (Test)", y_test, y_test_pred)
evaluate_model("Tuned Logistic Regression (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The tuned Logistic Regression model shows consistent performance on both training and test datasets, with accuracy around 79–80% and F1 scores close to 0.80. On the test set, it performs better at identifying the positive class (precision 0.87, recall 0.83), indicating good sensitivity toward the target condition, though it slightly underperforms on the negative class (precision 0.63, recall 0.71). The training set results mirror this balance, with precision and recall values near 0.80 for both classes, suggesting the model generalizes well without significant overfitting. Overall, the model demonstrates a reliable ability to distinguish between classes with balanced performance across metrics."""

# Perform 5-fold cross-validation on the logistic regression model
from sklearn.model_selection import cross_val_score

scores = cross_val_score(log_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("Logistic Regression Cross-Validated F1:", scores.mean())

"""**Interpretation:** The Logistic Regression model achieved a cross-validated F1 score of approximately 0.79, indicating consistent and balanced performance across the folds. This suggests the model is reliable in handling the classification task, maintaining good precision and recall while generalizing well on unseen data during cross-validation.

**XGBOOST**
"""

# Initialize and train XGBoost classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_res, y_train_res)

y_test_pred = xgb_model.predict(X_test_processed)
y_train_pred = xgb_model.predict(X_train_res)

evaluate_model("XGBoost (Test)", y_test, y_test_pred)
evaluate_model("XGBoost (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The XGBoost model The model achieved 75% accuracy and an F1 score of 0.74 on the test set, showing decent overall performance, especially for predicting the positive class (precision: 80%, recall: 87%). However, the model struggled with the negative class, with only 47% recall.

In contrast, the training set performance shows perfect accuracy and F1 score, indicating the model is likely overfitting-memorizing the training data but failing to generalize well on new/unseen data.
"""

# Initialize XGBoost with tuned hyperparameters for balanced complexity, regularization, and randomness.
xgb_model = XGBClassifier(
    learning_rate=0.05,
    n_estimators=500,
    max_depth=3,
    subsample=0.5,
    colsample_bytree=0.5,
    reg_alpha=10,
    reg_lambda=20,
    min_child_weight=3,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

xgb_model.fit(X_train_res, y_train_res)

y_train_pred = xgb_model.predict(X_train_res)
y_test_pred = xgb_model.predict(X_test_processed)

evaluate_model("Test", y_test, y_test_pred)
evaluate_model("Train", y_train_res, y_train_pred)

"""**Interpretation:** After hyperparameter tuning, the XGBoost model showed strong and balanced performance. On the test set, it achieved an accuracy of 80% and an F1 score of 0.80, reflecting consistent predictive power. The model maintained high precision (86%) and recall (87%) for the positive class, effectively identifying true positives.

On the training set, the accuracy and F1 score remained stable at 84%, indicating improved generalization and minimal overfitting. Overall, tuning enhanced the model’s ability to perform well across both training and unseen data, balancing sensitivity and specificity effectively.
"""

import warnings
warnings.filterwarnings('ignore')
scores = cross_val_score(xgb_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("XGBoost Cross-Validated F1:", scores.mean())

"""**Interpretation:** The XGBoost model achieved a strong cross-validated F1 score of 0.86, indicating high performance in balancing precision and recall across the training folds. This suggests that the model is effective at correctly identifying both positive and negative classes in the training data.

**RANDOM FOREST**
"""

# Train a Random Forest classifier with balanced class weights to address class imbalance.

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    class_weight='balanced',
    random_state=42
)

rf_model.fit(X_train_res, y_train_res)
y_train_pred = rf_model.predict(X_train_res)
y_test_pred = rf_model.predict(X_test_processed)

evaluate_model("Test Set (Random Forest)", y_test, y_test_pred)
evaluate_model("Training Set (Random Forest)", y_train_res, y_train_pred)

"""**Interpretation:** Random Forest model performs reasonably well on the test set, achieving an F1 score of 0.79, indicating good overall predictive performance. However, there is a slight imbalance in correctly identifying the minority class, as reflected in the lower F1 score for class 0.

On the training set, the model achieved perfect scores (F1 = 1.0), which strongly suggests overfitting-it memorized the training data but does not generalize as well to unseen data. This reinforces the need for hyperparameter tuning to improve generalization and reduce overfitting.
"""

rf_model = RandomForestClassifier(
    n_estimators=250,
    max_depth=6,
    min_samples_leaf=11,
    min_samples_split=10,
    max_features='sqrt',
    random_state=42
)

rf_model.fit(X_train_res, y_train_res)
y_test_pred = rf_model.predict(X_test_processed)
y_train_pred = rf_model.predict(X_train_res)

evaluate_model("Tuned Random Forest (Test)", y_test, y_test_pred)
evaluate_model("Tuned Random Forest (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The tuned Random Forest model demonstrates solid and balanced performance across both training and test sets. On the test set, it achieved an F1 score of 0.81, indicating reliable classification ability with good precision and recall. While the model performs slightly better for the majority class, it still maintains decent sensitivity for the minority class.

On the training set, the F1 score of 0.87 shows that the model fits the training data well without severe overfitting, especially compared to the untuned decision tree. Overall, the tuned Random Forest shows strong generalization.
"""

scores = cross_val_score(rf_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("Random Forest Cross-Validated F1:", scores.mean())

"""**Interpretation:** The cross-validated F1 score of 0.825 for the Random Forest model indicates strong and consistent performance across the training folds. This suggests that the model is effectively capturing patterns in the data while maintaining good generalizability. Its F1 score, balancing both precision and recall, especially in handling class imbalance and reducing the risk of overfitting.

**DECISION TREE**
"""

# Train a Decision Tree classifier

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train_res, y_train_res)

y_test_pred = dt_model.predict(X_test_processed)
y_train_pred = dt_model.predict(X_train_res)

evaluate_model("Decision Tree (Test)", y_test, y_test_pred)
evaluate_model("Decision Tree (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The Decision Tree model memorized the training data, achieving 100% accuracy, precision, recall, and F1 score on the training set. This indicates severe overfitting, as the model has learned the training examples exactly without generalizing. On the test set, performance dropped notably, with an accuracy of approximately 74% and an F1 score of 0.73. While the model still performs well identifying the positive class (1) - with precision of 79% and recall of 87% - it struggles with the negative class (0), showing low precision (58%) and recall (44%). The confusion matrix confirms many misclassifications for the negative class.

Overall, Decision Tree overfits the training data and lacks generalization, leading to reduced test accuracy and imbalance in class predictions.
"""

# Perform hyperparameter tuning on Decision Tree using GridSearchCV to find best parameters,
# then train with the best estimator and predict on train and test sets.

dt = DecisionTreeClassifier(random_state=42)
param_grid = {
    'max_depth': [7],
    'min_samples_split': [11],
    'min_samples_leaf': [9],
    'max_features': ['sqrt']
}
grid_search = GridSearchCV(
    estimator=dt,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=2
)
grid_search.fit(X_train_res, y_train_res)
print("Best parameters:", grid_search.best_params_)

best_dt = grid_search.best_estimator_

y_test_pred = best_dt.predict(X_test_processed)
y_train_pred = best_dt.predict(X_train_res)

evaluate_model("Tuned Decision Tree (Test)", y_test, y_test_pred)
evaluate_model("Tuned Decision Tree (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The tuned Decision Tree model demonstrated solid performance on both the training and test datasets. On the test set, it achieved an accuracy of approximately 78% with an F1 score of 0.78, showing effective classification overall. The model performed better on the positive class (1), with a precision of 82% and recall of 89%, indicating strong ability to identify students at risk. However, the negative class (0) showed lower performance, with precision at 67% and recall at 53%, suggesting the model struggles more to correctly identify low-risk individuals.

On the training set, the model performed well with an accuracy of 86% and an F1 score of 0.86, indicating good learning and generalization with balanced precision and recall for both classes. The higher performance on training compared to testing suggests some overfitting, but overall, the tuned Decision Tree is a reliable model for prediction, especially effective at detecting the positive class.

"""

scores = cross_val_score(dt_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("Decision Tree Cross-Validated F1:", scores.mean())

"""**Interpretation:** The Decision Tree model achieved a strong cross-validated F1 score of 0.83, indicating consistent and reliable performance across different training folds. This suggests the model effectively balances precision and recall, capturing important patterns in the data while generalizing well to unseen samples.

**SUPPORT VECTOR MACHINE(SVM)**
"""

# Compare SVM model performance using different kernels by training and evaluating accuracy on both training and test data,
# then plot the accuracies to visualize which kernel performs best.

kernels = ['linear', 'poly', 'rbf', 'sigmoid']

train_accuracies = []
test_accuracies = []

for kernel in kernels:
    svm = SVC(kernel=kernel, random_state=42)
    svm.fit(X_train_processed, y_train)

    train_pred = svm.predict(X_train_processed)
    test_pred = svm.predict(X_test_processed)

    train_accuracies.append(accuracy_score(y_train, train_pred))
    test_accuracies.append(accuracy_score(y_test, test_pred))

plt.figure(figsize=(8, 6))
plt.plot(kernels, train_accuracies, marker='o', label='Training Accuracy')
plt.plot(kernels, test_accuracies, marker='s', label='Test Accuracy')
plt.title('SVM Accuracy Comparison Across Kernels')
plt.xlabel('Kernel Type')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()
plt.grid(True)
plt.show()

"""**Interpretation:** The graph shows the training and test accuracy of a Support Vector Machine (SVM) model using different kernel types: linear, polynomial (poly), radial basis function (rbf), and sigmoid. Among these, the RBF kernel yields the highest training accuracy, closely followed by the polynomial kernel. However, the test accuracy remains relatively stable across the linear, poly, and rbf kernels, with the RBF kernel performing slightly better overall. The sigmoid kernel, in contrast, shows a noticeable drop in both training and test accuracy, indicating poorer performance and possibly underfitting. The relatively small gap between training and test accuracies for linear, poly, and RBF kernels suggests that these models generalize well, while the sigmoid kernel appears to be the least effective for this dataset. Overall, the RBF kernel provides the best trade-off between training and test performance."""

svm_model = SVC(random_state=42)
svm_model.fit(X_train_res, y_train_res)
y_train_pred = svm_model.predict(X_train_res)
y_test_pred = svm_model.predict(X_test_processed)
evaluate_model("SVM (Test)", y_test, y_test_pred)
evaluate_model("SVM (Train)", y_train_res, y_train_pred)

"""**Interpretation:** The SVM model shows a noticeable difference in performance between the training and test sets. On the test set, the model achieved an accuracy and F1 score of approximately 77.6%, reflecting moderate predictive ability. It performed well for the positive class (1) with 84% precision and recall, but showed lower effectiveness on the negative class (0), with both precision and recall around 62%, indicating some misclassification.

On the training set, the model performed significantly better, with an accuracy of 85.8% and an F1 score of 0.86, demonstrating strong ability to classify both classes effectively (precision and recall above 83% for both). The performance gap between training and test suggests potential overfitting, indicating that further tuning may be necessary to improve generalization on unseen data.
"""

svm = SVC(probability=True, random_state=42)
param_grid = {
    'C': [10],
    'kernel': ['linear'],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(
    estimator=svm,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train_processed, y_train)
print("Best params:", grid_search.best_params_)
best_svm = grid_search.best_estimator_

y_test_pred = best_svm.predict(X_test_processed)
y_train_pred = best_svm.predict(X_train_processed)

evaluate_model("Tuned SVM (Test)", y_test, y_test_pred)
evaluate_model("Tuned SVM (Train)", y_train, y_train_pred)

"""**Interpretation:** After tuning, the SVM model demonstrated solid and balanced performance on both the training and test sets. On the test set, it achieved an accuracy of approximately 78% and an F1 score of 0.79, indicating effective classification ability. The model performed particularly well on the positive class (1), with a precision of 88% and recall of 80%, showing strong capability in identifying students at higher mental health risk. However, the negative class (0) had lower precision and recall (61% and 74%, respectively), suggesting some misclassification of low-risk individuals.

On the training set, the model showed similar performance, with an accuracy of 80% and an F1 score of 0.81, indicating good generalization without substantial overfitting. The recall for the negative class improved to 80%, while the positive class maintained high precision (91%) and recall (80%). Overall, the tuned SVM is a reliable classifier that balances sensitivity and specificity.
"""

param_range = np.logspace(-3, 3, 7)
svm = make_pipeline(StandardScaler(), SVC(kernel='rbf'))

# Compute validation curve
train_scores, test_scores = validation_curve(
    svm, X_train_processed, y_train,
    param_name="svc__C",
    param_range=param_range,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure(figsize=(8, 6))
plt.plot(param_range, train_mean, label="Training score", color="blue", marker='o')
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color="blue")

plt.plot(param_range, test_mean, label="Cross-validation score", color="green", marker='s')
plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color="green")

plt.xscale('log')
plt.xlabel("Parameter C (log scale)")
plt.ylabel("Accuracy")
plt.title("Validation Curve for SVM (RBF Kernel)")
plt.legend(loc="best")
plt.grid(True)
plt.show()

"""**Interpretation**: The validation curve for the SVM model with an RBF kernel shows the relationship between the regularization parameter
C and model accuracy. As
C increases, the training accuracy steadily improves, eventually reaching near-perfect levels, indicating the model is fitting the training data very closely. However, this comes at the expense of generalization, as the cross-validation accuracy initially rises and then declines, suggesting overfitting at higher values of
C. The cross-validation accuracy peaks around
C=1, which represents the optimal balance between bias and variance. At very low values of
C, both training and validation accuracies are low, indicating underfitting due to overly strong regularization. Thus, the optimal value of C=1 yields the best generalization performance for the SVM model.
"""

scores = cross_val_score(svm_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("SVM Cross-Validated F1:", scores.mean())

"""**Interpretation:** The SVM model achieved a strong cross-validated F1 score of 0.83, indicating robust and consistent performance across training folds. This reflects its ability to balance precision and recall effectively, suggesting good generalization to unseen data and strong predictive power for this classification task.

**K-NEAREST NEIGHBORS**
"""

# Initialize and train K-Nearest Neighbors classifier with 5 neighbors,
# then predict outcomes on both training and test data.

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_processed, y_train)

y_train_pred = knn_model.predict(X_train_processed)
y_test_pred = knn_model.predict(X_test_processed)

evaluate_model("Test Set (KNN)", y_test, y_test_pred)
evaluate_model("Training Set (KNN)", y_train, y_train_pred)

"""**Interpretation:** The K-Nearest Neighbors (KNN) model achieved an accuracy of ~79.3% and an F1 score of ~0.79 on the test set, showing good overall performance. It was particularly strong in identifying individuals with higher mental health severity (class 1), achieving a precision of 84% and recall of 88%. However, the model struggled with class 0, where precision dropped to 67% and recall to 59%, indicating a tendency to misclassify low-severity cases.

On the training set, the model performed better with 86.2% accuracy and an F1 score of ~0.86. It maintained high performance for class 1 (recall: 94%), but again, class 0 recall was lower at 68%. This suggests that while the model generalizes fairly well, it consistently favors the majority class.
"""

# Evaluate KNN performance over a range of k values (1 to 20),
# record training and test accuracies for each k,
# then plot accuracies vs. number of neighbors to help select the optimal k.

k_values = range(1, 21)
train_accuracies = []
test_accuracies = []
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_processed, y_train)

    train_pred = knn.predict(X_train_processed)
    test_pred = knn.predict(X_test_processed)

    train_accuracies.append(accuracy_score(y_train, train_pred))
    test_accuracies.append(accuracy_score(y_test, test_pred))
plt.figure(figsize=(10, 6))
plt.plot(k_values, train_accuracies, marker='o', label='Training Accuracy')
plt.plot(k_values, test_accuracies, marker='s', label='Test Accuracy')
plt.title('KNN Accuracy vs. K')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.legend()
plt.grid(True)
plt.show()

"""**Interpretation:** The graph displays the accuracy of the K-Nearest Neighbors (KNN) model on both the training and test sets across different values of K, the number of neighbors. The red line highlights the tuned value of K = 9, which was selected as the optimal value after hyperparameter tuning. At K = 9, the test accuracy is relatively high and stable, while the gap between training and test accuracy is reasonably small, indicating a good trade-off between bias and variance.

The plot also shows that lower values of K (e.g., K = 1–3) result in very high training accuracy but significantly lower and unstable test accuracy, a sign of overfitting. As K increases, training accuracy decreases due to increased generalization, and test accuracy stabilizes. The selection of K = 9 minimizes overfitting while maintaining consistent performance on unseen data, suggesting that the model is now better tuned for generalization after hyperparameter optimization.
"""

# Perform hyperparameter tuning for KNN using GridSearchCV,
# searching for the best number of neighbors, weighting method, and distance metric (p),
# then fit the best model and make predictions on train and test sets.

knn = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [9],
    'weights': ['uniform', 'distance'],
    'p': [1]
}

grid_search = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train_processed, y_train)
print("Best params:", grid_search.best_params_)
best_knn = grid_search.best_estimator_

y_test_pred = best_knn.predict(X_test_processed)
y_train_pred = best_knn.predict(X_train_processed)

evaluate_model("Tuned KNN (Test)", y_test, y_test_pred)
evaluate_model("Tuned KNN (Train)", y_train, y_train_pred)

"""**Interpretation:** After tuning, the KNN model shows improved performance, particularly on the test set where accuracy rises to approximately 82% and the F1 score reaches 0.88. The model demonstrates strong precision and recall for class 1, indicating high effectiveness in identifying individuals with higher severity. Class 0 performance also improves slightly in precision, though recall remains lower. On the training set, the model maintains high accuracy (around 87%) with strong recall and F1 scores for class 1, and moderate performance for class 0. Overall, tuning enhanced generalization, particularly for the minority class, though some imbalance in class 0 prediction persists."""

k_values = range(1, 21)
train_accuracies = []
test_accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_processed, y_train)

    train_pred = knn.predict(X_train_processed)
    test_pred = knn.predict(X_test_processed)

    train_accuracies.append(accuracy_score(y_train, train_pred))
    test_accuracies.append(accuracy_score(y_test, test_pred))

best_k = 9  # From your grid_search param grid

plt.figure(figsize=(10, 6))
plt.plot(k_values, train_accuracies, marker='o', label='Training Accuracy')
plt.plot(k_values, test_accuracies, marker='s', label='Test Accuracy')
plt.axvline(x=best_k, color='red', linestyle='--', label=f'Tuned k = {best_k}')
plt.title('KNN Accuracy vs. Number of Neighbors (K)')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.legend()
plt.grid(True)
plt.show()

"""**Interpretation:** The graph displays the accuracy of the K-Nearest Neighbors (KNN) model on both the training and test sets across different values of K, the number of neighbors. The red dashed line highlights the tuned value of K = 9, which was selected as the optimal value after hyperparameter tuning. At K = 9, the test accuracy is relatively high and stable, while the gap between training and test accuracy is reasonably small, indicating a good trade-off between bias and variance.

The plot also shows that lower values of K (e.g., K = 1-3) result in very high training accuracy but significantly lower and unstable test accuracy, a sign of overfitting. As K increases, training accuracy decreases due to increased generalization, and test accuracy stabilizes. The selection of K = 9 minimizes overfitting while maintaining consistent performance on unseen data, suggesting that the model is now better tuned for generalization after hyperparameter optimization.
"""

scores = cross_val_score(knn_model, X_train_res, y_train_res, cv=5, scoring='f1')
print("KNN Cross-Validated F1:", scores.mean())

"""**Interpretation:** The KNN model achieved a cross-validated F1 score of 0.78, showing consistent and balanced performance across training folds. This indicates good generalization and a reliable balance between precision and recall

**ARTIFICIAL NEURAL NETWORKS**
"""

# Set seeds and environment variables for reproducibility and CPU-only execution.
# Build a simple neural network with one hidden Dense layer (32 units, ReLU) and an output layer with sigmoid activation.
# Compile model with Adam optimizer, binary cross-entropy loss, and accuracy metric.
# Train the model for 20 epochs without shuffling the training data.
# Predict probabilities on train and test data, then convert probabilities to binary class predictions using 0.5 threshold.

seed = 42
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = ''

random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train_processed.shape[1],)),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_processed, y_train,
          epochs=20,
          batch_size=32,
          verbose=1,
          shuffle=False)

y_train_probs = model.predict(X_train_processed).ravel()
y_test_probs = model.predict(X_test_processed).ravel()

y_train_pred = (y_train_probs > 0.5).astype(int)
y_test_pred = (y_test_probs > 0.5).astype(int)

evaluate_model("ANN (Test)", y_test, y_test_pred)
evaluate_model("ANN (Train)", y_train, y_train_pred)

"""**Interpretation:** the Artificial Neural Network (ANN) demonstrated good performance with an accuracy of 81% on the test set and an F1 score of ~0.81, showing strong ability to identify mental health risk cases. The model performed especially well on the positive class (label 1), achieving precision of 85% and recall of 89%, which means it effectively identified most students at risk.

However, the model struggled more with the negative class (label 0), with a precision of 70% and recall of 62%, suggesting a moderate number of false positives. On the training set, performance was slightly higher (accuracy of ~84%), indicating mild overfitting.
"""

!pip install keras-tuner --quiet

# Use Keras Tuner for tuning units and learning rate in a simple NN.
# Set seeds for reproducibility and run RandomSearch for 5 trials.
# Train with validation split, get best model, predict on train/test with 0.5 threshold.

import kerastuner as kt
from kerastuner.tuners import RandomSearch
seed = 42
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = ''

random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

def build_model(hp):
    model = Sequential()
    units = hp.Int('units', 16, 128, step=16)
    model.add(Dense(units=units, activation='relu', input_shape=(X_train_processed.shape[1],)))
    model.add(Dense(1, activation='sigmoid'))
    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model
tuner = RandomSearch(build_model,
                     objective='val_accuracy',
                     max_trials=5,
                     executions_per_trial=1,
                     directory='my_dir',
                     project_name='simple_tuning')

tuner.search(X_train_processed, y_train,
             epochs=20,
             validation_split=0.2,
             batch_size=32,
             shuffle=False)

best_model = tuner.get_best_models(num_models=1)[0]

y_train_pred = (best_model.predict(X_train_processed).ravel() > 0.5).astype(int)
y_test_pred = (best_model.predict(X_test_processed).ravel() > 0.5).astype(int)

evaluate_model("Test Set", y_test, y_test_pred)
evaluate_model("Training Set", y_train, y_train_pred)

"""**Interpretation:** After hyperparameter tuning, the ANN model demonstrated improved generalization and balanced performance. On the test set, it achieved an accuracy of ~81.9% and an F1 score of ~0.82, indicating reliable prediction capability. The model maintained strong performance on the positive class (label 1) with a precision of 86% and recall of 89%, which is crucial for identifying individuals at mental health risk. Performance on the negative class (label 0) slightly improved post-tuning, with precision increasing to 71% and recall to 65%, reducing false positives.

On the training set, the model yielded an accuracy of ~84% and F1 score of ~0.84, closely aligned with the test metrics, which suggests reduced overfitting. The confusion matrix and classification report show a well-balanced performance across both classes
"""

pip install -U scikeras scikit-learn

from scikeras.wrappers import KerasClassifier
from tensorflow.keras.layers import Input

def create_ann_model():
    model = Sequential()
    model.add(Input(shape=(X_train_processed.shape[1],)))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

ann_clf = KerasClassifier(model=create_ann_model, epochs=20, batch_size=32, verbose=0)

scores = cross_val_score(ann_clf, X_train_processed, y_train, cv=5, scoring='f1')
print("ANN Cross-Validated F1:", np.mean(scores))

"""**Interpretation:** The ANN model achieved a strong cross-validated F1 score of 0.86, indicating effective and balanced performance. Its ability to learn complex patterns from the data makes it well-suited for this classification task. The consistent results across folds confirm its generalizability and reliability."""

# Plot ROC curves for all models on test data to compare their AUC performance visually.
# Predict probabilities for positive class, compute ROC metrics, and display curves with AUC scores.

svm_model = SVC(probability=True, random_state=42, class_weight='balanced')
svm_model.fit(X_train_res, y_train_res)

y_score_log = log_model.predict_proba(X_test_processed)[:, 1]
y_score_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]
y_score_rf = rf_model.predict_proba(X_test_processed)[:, 1]
y_score_dt = dt_model.predict_proba(X_test_processed)[:, 1]
y_score_svm = svm_model.predict_proba(X_test_processed)[:, 1]
y_score_knn = knn_model.predict_proba(X_test_processed)[:, 1]
y_score_ann = y_test_probs

plt.figure(figsize=(10, 8))

def plot_roc_curve(y_true, y_scores, label):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.3f})')

plot_roc_curve(y_test, y_score_log, 'Logistic Regression')
plot_roc_curve(y_test, y_score_xgb, 'XGBoost')
plot_roc_curve(y_test, y_score_rf, 'Random Forest')
plot_roc_curve(y_test, y_score_dt, 'Decision Tree')
plot_roc_curve(y_test, y_score_svm, 'SVM')
plot_roc_curve(y_test, y_score_knn, 'KNN')
plot_roc_curve(y_test, y_score_ann, 'ANN')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - All Models on Test Set')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""**Interpretation:** The ROC curve plot compares the performance of seven classification models on the test set using the Area Under the Curve (AUC) metric. The Artificial Neural Network (ANN) achieved the highest AUC of 0.826, indicating the best discriminative ability among all models. This is closely followed by XGBoost (AUC = 0.824) and Logistic Regression (AUC = 0.822), both of which also demonstrate strong classification performance. Random Forest also performed well with an AUC of 0.802. In contrast, the Decision Tree model showed the weakest performance with an AUC of only 0.654, suggesting limited predictive accuracy. The Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) models achieved moderate results with AUCs of 0.774 and 0.768, respectively. Overall, the ROC curves and AUC values suggest that ANN, XGBoost, and Logistic Regression are the most effective models for predicting anxiety severity in this dataset.

**COMPARISON** **TABLES**

**Model Performance After Tuning**
"""

def get_metrics(name, y_true, y_pred, y_prob=None):
    try:
        auc = roc_auc_score(y_true, y_prob) if y_prob is not None else "N/A"
    except:
        auc = "N/A"

    return {
        "Model": name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1 Score": f1_score(y_true, y_pred),
        "AUC": auc
    }

results = []

# Logistic Regression
log_y_pred = log_model.predict(X_test_processed)
log_y_prob = log_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Logistic Regression", y_test, log_y_pred, log_y_prob))

# Xgboost
xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_res, y_train_res)
xgb_y_pred = xgb_model.predict(X_test_processed)
xgb_y_prob = xgb_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("XGBoost", y_test, xgb_y_pred, xgb_y_prob))

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_res, y_train_res)
rf_y_pred = rf_model.predict(X_test_processed)
rf_y_prob = rf_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Random Forest", y_test, rf_y_pred, rf_y_prob))

# Decision Tree
dt_y_pred = dt_model.predict(X_test_processed)
dt_y_prob = dt_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Decision Tree", y_test, dt_y_pred, dt_y_prob))

# SVM
svm_model = SVC(probability=True, random_state=42)
svm_model.fit(X_train_res, y_train_res)
svm_y_pred = svm_model.predict(X_test_processed)
svm_y_prob = svm_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("SVM", y_test, svm_y_pred, svm_y_prob))

# KNN
knn_y_pred = knn_model.predict(X_test_processed)
knn_y_prob = knn_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("KNN", y_test, knn_y_pred, knn_y_prob))

# ANN
y_test_probs = model.predict(X_test_processed).ravel()
y_test_pred = (y_test_probs > 0.5).astype(int)
results.append(get_metrics("ANN", y_test, y_test_pred, y_test_probs))

results_df = pd.DataFrame(results)
results_df = results_df.round(4)

# Sort by F1 Score in descending order (best models first)
results_df = results_df.sort_values(by="F1 Score", ascending=False)

print("\n📊 Model Performance Comparison Table:")
results_df

"""**Model Performance After Tuning**

**Interpretation:** The Artificial Neural Network (ANN) outperformed other models with the highest F1 score (0.8690), indicating strong predictive capability and balance between precision (0.8488) and recall (0.8902). Both the K-Nearest Neighbors (KNN) and Random Forest models followed closely, each achieving an F1 score of 0.8571, suggesting consistent performance across different ensemble and instance-based methods. Logistic Regression maintained high precision (0.8718) and achieved a balanced F1 score (0.8500), demonstrating solid baseline performance. The Support Vector Machine (SVM) showed equal precision and recall (0.8415), resulting in an identical F1 score, reflecting stable classification but slightly lower overall performance. XGBoost, while commonly known for strong performance, lagged slightly behind with an F1 score of 0.8304 and an AUC of 0.7231. The Decision Tree model had the lowest F1 score (0.8256) and the weakest AUC (0.6535), suggesting susceptibility to overfitting and limited generalization. Overall, ANN, KNN, and Random Forest showed the strongest pre-tuning performance, while tree-based models like Decision Tree and XGBoost were less effective in their default configurations.
"""

def get_metrics(name, y_true, y_pred, y_prob=None):
    try:
        auc = roc_auc_score(y_true, y_prob) if y_prob is not None else "N/A"
    except:
        auc = "N/A"
    return {
        "Model": name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1 Score": f1_score(y_true, y_pred),
        "AUC": auc
    }

results = []

# Logistic Regression (tuned)
log_test_pred = best_log_model.predict(X_test_processed)
log_test_prob = best_log_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned Logistic Regression", y_test, log_test_pred, log_test_prob))

# XGBoost (tuned)
xgb_test_pred = xgb_model.predict(X_test_processed)
xgb_test_prob = xgb_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned XGBoost", y_test, xgb_test_pred, xgb_test_prob))

# Random Forest (tuned)
rf_test_pred = rf_model.predict(X_test_processed)
rf_test_prob = rf_model.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned Random Forest", y_test, rf_test_pred, rf_test_prob))

# Decision Tree (tuned)
dt_test_pred = best_dt.predict(X_test_processed)
dt_test_prob = best_dt.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned Decision Tree", y_test, dt_test_pred, dt_test_prob))

# SVM (tuned)
svm_test_pred = best_svm.predict(X_test_processed)
svm_test_prob = best_svm.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned SVM", y_test, svm_test_pred, svm_test_prob))

# KNN (tuned)
knn_test_pred = best_knn.predict(X_test_processed)
knn_test_prob = best_knn.predict_proba(X_test_processed)[:, 1]
results.append(get_metrics("Tuned KNN", y_test, knn_test_pred, knn_test_prob))

# ANN (tuned)
y_test_probs_tuned = best_model.predict(X_test_processed).ravel()
y_test_pred_tuned = (y_test_probs_tuned > 0.5).astype(int)
results.append(get_metrics("Tuned ANN", y_test, y_test_pred_tuned, y_test_probs_tuned))

results_df = pd.DataFrame(results).round(4)

# Sort by F1 Score in descending order (best models first)
results_df = results_df.sort_values(by="F1 Score", ascending=False)

print("\n Tuned Models Performance Comparison Table:")
results_df

"""**Interpretation:** Among the tuned models, K-Nearest Neighbors (KNN) achieved the highest F1 score (0.8786), indicating strong overall classification performance, particularly in balancing precision and recall. The Artificial Neural Network (ANN) followed closely with an F1 score of 0.8743 and the highest AUC (0.8350), suggesting excellent discriminative ability between anxiety severity classes. The Random Forest and Decision Tree models also performed well, both with F1 scores above 0.85, demonstrating strong predictive power after tuning. Logistic Regression and SVM showed slightly lower F1 scores (0.8500 and 0.8408, respectively), but both maintained high precision and AUC values, indicating reliability in identifying true positives. XGBoost, although widely recognized for its performance, ranked lowest among the tuned models in this context with an F1 score of 0.8304 and the lowest accuracy (0.75), suggesting it was less effective for this particular dataset and problem setup. Overall, tuning substantially improved all models, with KNN and ANN emerging as top performers for predicting anxiety severity.

**BEST** **MODEL(KNN)**
"""

predictors = [
    'sex', 'Age', 'Race', 'Previously_Diagnosed_Depression',
    'Total_Health_Issues_Score', 'In_Therapy_for_Depression',
    'Hopeless', 'Very sad', 'Depression_Severity_Score',
    'Overwhelmed', 'Exhausted (not physical)',
    'Depressed - hard to function', 'Attempted suicide'
]
target = 'Anxiety_Severity'

model_input_df = data[predictors + [target]].dropna()

X = model_input_df[predictors]
y = model_input_df[target]

categorical_cols = ['sex', 'Race']
numeric_cols = [
    'Age', 'Previously_Diagnosed_Depression', 'In_Therapy_for_Depression',
    'Total_Health_Issues_Score', 'Hopeless', 'Very sad', 'Depression_Severity_Score',
    'Overwhelmed', 'Exhausted (not physical)', 'Depressed - hard to function', 'Attempted suicide'
]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),
    ("num", StandardScaler(), numeric_cols)
])
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_processed, y_train)

knn = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [9],
    'weights': ['uniform', 'distance'],
    'p': [1]
}

grid_search = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train_processed, y_train)
print("Best params:", grid_search.best_params_)
best_knn = grid_search.best_estimator_

y_test_pred = best_knn.predict(X_test_processed)
y_train_pred = best_knn.predict(X_train_processed)

evaluate_model("Tuned KNN (Test)", y_test, y_test_pred)
evaluate_model("Tuned KNN (Train)", y_train, y_train_pred)

"""**Interpretation:** Based on the evaluation metrics, the Tuned K-Nearest Neighbors (KNN) model demonstrated the strongest overall performance among all models. On the test set, it achieved an impressive accuracy of 81.9%, with a precision of 0.81, recall of 0.82, and F1 score of 0.81, indicating a well-balanced ability to correctly identify both classes, especially the majority class with mild to severe anxiety. The confusion matrix shows that the model correctly classified 76 out of 82 anxiety-positive cases, reflecting high sensitivity (recall of 0.93) for identifying at-risk individuals-crucial in mental health prediction tasks.

On the training set, the tuned KNN model maintained a high accuracy of 86.6%, with similarly strong precision, recall, and F1 scores (all above 0.86), suggesting the model generalizes well without significant overfitting. While some misclassification of the minimal-anxiety class (class 0) occurred, the weighted average metrics remained robust, and the macro average F1 score (0.76 on test) still outperformed several other tuned models. These results, combined with its simplicity and interpretability, position the tuned KNN model as the most effective and reliable classifier for predicting anxiety severity in this dataset.
"""

joblib.dump(best_knn, 'knn_model.pkl')
joblib.dump(preprocessor, 'preprocessor.pkl')

model = joblib.load("knn_model.pkl")
preprocessor = joblib.load("preprocessor.pkl")